# 1.1 统计学习

1）统计学习（statistical learning）：是关于计算机基于数据构建概论统计模型并运用模型对数据进行预测与分析的一门学科。统计学习也称为统计机器学习

2）统计学习的特点：

    平台：计算机及网络
    
    研究对象：数据
    
    目的：对数据进行预测
    
    中心：方法
    
    交叉学科：概率论、统计学、信息论、计算理论、最优化理论及计算机科学
   
3）统计学习的对象：数据

基本假设是同类数据具有一定的统计规律性，同类数据是指具有某种共同性质的数据。

从数据出发，提取数据的特征，抽象出数据的模型，发现模型的知识，又回到对数据的分析与预测钟去。

4）统计学习的目的：对数据的预测与分析，特别是对未知新数据的预测与分析。

对数据的预测是通过构建概率统计模型实现的

5）统计学习的方法：基于数据构建概率统计模型从而对数据进行预测与分析。

统计学方法包括：模型的假设空间、模型选择的准则以及模型学习的算法

三要素：模型、策略、算法

实现步骤如下：

    （1）得到一个有限的训练数据集合
    
    （2）确定包含所有可能模型的假设空间，即学习模型的集合
    
    （3）确定模型选择的准则，即学习的策略
    
    （4）实现求解最优模型的算法，即学习的算法
    
    （5）通过学习算法选择最优模型
    
    （6）利用学习的最优模型对新数据进行预测或分析
    
6）统计学习的研究：统计学习方法、统计学习理论、统计学习应用

7）统计学习的重要性体现：
    
    （1）统计学习是处理海量数据的有效方法
    
    （2）统计学习是计算机智能化的有效手段
    
    （3）统计学习是计算机科学发展的一个重要组成部分
    
# 1.2 统计学习的分类

## 1.2.1 基本分类
    
    一般包括监督学习、无监督学习、强化学习。有时包括半监督学习、主动学习。
    
### 1.2.1.1 监督学习
   
   监督学习（supervised learning）是指从标注数据钟学习预测模型的机器学习问题。标注数据表示输入输出的对应关系，预测模型对给定的输入产生相应的输出。
   
   监督学习的本质是学习输入到输出的映射的统计规律。
   
   （1）输入空间、特征空间和输出空间
   
        输入空间：输入所有可能取值的集合
        
        输出空间：输出所有可能取值的集合
        
        特征空间：所有特征向量存在的空间
        
        输入变量与输出变量均为连续变量的预测问题称为回归问题；输出变量为有限个离散变量的预测问题称为分类问题；输入变量与输出变量均为变量序列的预测问题称为标注问题
        
   （2）联合概率分布：监督学习假设输入与输出的随机变量X和Y遵循联合概率分布P(X,Y)。P(X,Y)表示分布函数或分布密度函数
   
   （3）假设空间：模型属于由输入空间到输出空间的映射的集合，这个集合就是假设空间（hypothesis space）。假设空间意的确定意味着学习的范围的确定。
   
   （4）问题的形式化：利用训练数据集学习一个模型，再用模型对测试样本集进行预测
   
### 1.2.1.2 无监督学习

   无监督学习（unsupervised learning）是指从无标注数据中学习预测模型的机器问题。无标注数据是自然得到的数据，预测模型表示数据的类别、转换或概率。
   
   无监督学习的本质是学习数据钟的统计规律或潜在结构。
   
### 1.2.1.3 强化学习

   强化学习（reinforcement learning）是指智能系统在与环境的连续互动中学习最优行为策略的机器学习问题
   
   强化学习的本质是学习最优的序贯决策。智能系统的目标不是短期奖励的最大化，而是长期积累奖励的最大化。强化学习过程中，系统不断地试错，以达到学习最优策略的目的。
    
### 1.2.1.4 半监督学习与主动学习
    
    半监督学习（semi-supervised learning）是指利用标注数据和未标注数据学习预测模型的机器学习问题。通常有少量标注数据、大量未标注数据，因为标注数据的构建往往需要人工，成本较高，未标注数据的收集不需太多成本。
    
    半监督学习旨在利用未标注数据中的信息，辅助标注数据，进行监督学习，以较低的成本达到较好的学习效果。
    
    主动学习（active learning）是指机器不断主动给出实例让教师进行标注，然后利用标注数据学习预测模型的机器学习问题。通常的监督学习使用给定的标注数据，往往是随机得到的，可以看作是“被动学习”，主动学习的目标是找出对学习最有帮助的实例让教师标注，以较小的标注代价，达到较好的学习效果。
    
## 1.2.2 按模型分类

### 1.2.2.1 概率模型与非概率模型

#### 1.2.2.1.1 概率模型

监督学习中，概率模型取条件概率分布形式P(y|x)，其中x是输入，y是输出；无监督学习中，概率模型取条件概率分布形式P(z|x)或P(x|z)，其中x是输入，z是输出。
    
概率模型包含：决策树、朴素贝叶斯、隐马尔可夫模型、条件随机场、概率潜在语义分析、潜在狄利克雷分配、高斯混合模型、逻辑斯谛回归
    
代表：概率图模型（probabilistic graphical model），是联合分布由有向图或无向图表示的概率模型，而联合概率分布可以根据图的结构分解为因子乘积的形式。无论模型如何复杂，均可用最基本的加法和乘法规则来进行概率推理。

概率图模型：贝叶斯网络、马尔可夫随机场、条件随机场 

#### 1.2.2.1.2 非概率模型

监督学习中，非概率模型取函数形式y=f(x)，其中x是输入，y是输出；无监督学习中，非概率模型取函数形式z=g(x)，其中x是输入，z是输出。
    
非概率模型包含：感知机、支持向量机、k近邻、AdaBoost、k均值、潜在语义分析、神经网络、逻辑斯谛回归

#### 1.2.2.1.3 概率模型和非概率模型的区别

条件概率分布P(y|x)和函数f(x)可以相互转化（条件概率分布P(z|x)和函数z=g(x)同样可以）。

具体地，条件概率分布最大化后得到函数，函数归一化之后得到条件概率分布。

所以概率模型和非概率模型的区别不在于输入与输出之间的映射，而是在于概率的模型结构。

概率模型一定可以表示为联合概率分布的形式，而非概率模型则不一定存在这样的联合概率分布
    
### 1.2.2.2 线性模型和非线性模型

非概率模型可以分为线性模型和非线性模型。如果函数为y=f(x)或z=g(x)，则称模型为线性模型，否则称为非线性模型

线性模型：线性支持向量机、k近邻、k均值、潜在语义分析

非线性模型：支持向量机、AdaBoost、神经网络

### 1.2.2.3 参数化模型和非参数化模型

参数化模型假设模型参数的维度固定，模型可以由有限维度参数完全刻画；非参数化模型假设模型参数的维度不固定或者说无穷大，随着训练数据量的增加而不断增大。

参数化模型：感知机、朴素贝叶斯、逻辑斯谛回归、k均值、高斯混合模型

非参数化模型：决策树、支持向量机、AdaBoost、k近邻、潜在语义分析、概率潜在语义分析、潜在狄利克雷分配
    
## 1.2.3 按算法分类

在线学习（一个样本输入）与批量学习（多个样本输入）

## 1.2.4 按技巧分类

### 1.2.4.1 贝叶斯学习

又称贝叶斯推理（Bayesian inference），是统计学、机器学习中的重要方法。其主要想法是：在概率模型的学习和推理中，利用贝叶斯定理，计算在给定数据条件下模型的条件概率，即后验概率，并应用这个原理进行模型的估计，以及对数据的预测。

模型估计中，估计整个后验概率分布P(θ|D)，通常取后验概率的最大的模型。

### 1.2.4.2 核方法
    
核方法（kernel method）是使用核函数表示和学习非线性模型的一种机器学习方法，可以用于监督学习和无监督学习。

核函数支持向量机、核PCA、核k均值
    
# 1.3 统计学习方法三要素

方法 = 模型 + 策略 + 算法

## 1.3.1 模型（构建模型）

模型就是所要学习的条件概率分布或决策函数。模型的假设空间（hypothesis space）包含所有可能的条件概率分布或决策函数。
    
假设空间可以定义为决策函数的集合，F={f|Y = f(x)}通常是由一个参数向量决定的函数族F={f|Y = fθ(x)}，参数向量θ取值于n维欧式空间Rn，称为参数空间；条件概率分布亦然。

由决策函数表示的模型为非概率模型，由条件概率分布表示的模型为概率模型。

## 1.3.2 策略（选取最优模型）

### 1.3.2.1 损失函数和风险函数

损失函数：描述输出的预测值与真实值的一致程度，度量预测错误的程度

常见损失函数：

    （1）0-1损失函数：L(Y,f(x)) = 1 [Y ≠ f(x)]; 0 [Y = f(x)]
    
    （2）平方损失函数：L(Y, f(x)) = (Y - f(x))²
    
    （3）绝对损失函数：L(Y, f(x)) = |Y - f(x)|
    
    （4）对数损失函数或对数似然损失函数：L(Y, P(Y|X)) = -logP(Y|X)
    
1）风险函数（risk function）或期望损失（expected loss）：理论上模型f(x)关于联合分布P(X,Y)的平均意义下的损失

2）经验风险（empirical risk）或经验损失（empirical loss）：模型f(x)关于训练数据集的平均损失

3）期望风险（expected risk）是模型关于联合分布的期望损失，经验风险是关于训练样本集的平均损失。当样本容量趋向无穷时，经验风险趋于期望风险；但由于现实中训练样本数目有限，所以不能使用经验风险来估计期望风险。

### 1.3.2.2 经验风险最小化与结构风险最小化

1）经验风险最小化（empirical risk minimization，ERM）：认为经验风险最小的模型就是最优的模型。

当样本容量足够大时，经验风险最小化策略能保证很好的学习效果；但样本容量不足时，效果未必很好。

2）结构风险最小化（structural risk minimization，SRM）：认为结构风险最小的模型就是最优的模型，是为了防止过拟合而提出来的策略。结构风险最小化等价于正则化。

在经验风险上加上表示模型复杂度的正则化项（regularizer）或惩罚项






















































