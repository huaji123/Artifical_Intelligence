1.sigmoid函数
g(z) = 1 / (1 + np.exp(-x))
特点：
    ①易造成梯度消失
    ②输出非0均值（非0为中心），收敛慢
    ③幂运算复杂，训练时间长

2.tanh函数
g(z) = (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))
特点：
    ①输出值是0均值（0为中心）
    ②易造成梯度消失
    ③幂运算复杂，训练时间长
    
3.ReLU(修正线性单元)
g(z) = Max(0,x)
优点：
    ①解决了梯度消失问题（正区间）
    ②只需判断输入是否大于0，计算速度快
    ③收敛速度远快于sigmoid和tanh
缺点：
    ①输出非0均值，收敛慢
    ②Dead ReLu问题：某些神经元可能永远不被激活，导致相应参数永远不能被更新

4.Leaky ReLu
g(z) = Max(0.01z, z)
特点：
    通过把x的非常小的线性分量给予负输入0.01x来调整负值零梯度问题
    继承ReLU的所有有点，不会产生Dead ReLU问题
    函数范围是负无穷到正无穷
    
5.Softplus:平滑版的ReLU函数
g(x) = log(1 + exp(x))

6.ELU函数
ELU(x) = x(x > 0), α(exp(x) - 1)(x <= 0)
特点：
    没有Dead ReLU问题，输出的平均值接近0，以0为中心。
    ELU 通过减少偏置偏移的影响，使正常梯度更接近于单位自然梯度，从而使均值向零加速学习。
    ELU函数在较小的输入下会饱和至负值，从而减少前向传播的变异和信息。
    ELU函数的计算强度更高。与Leaky ReLU类似，尽管理论上比ReLU要好，但目前在实践中没有充分的证据表明ELU总是比ReLU好。

7.Parametric ReLu：ReLu的改进版本
PReLU(x) = x(x > 0), αix (x <= 0)
    如果alpha_i=0，则PReLU(x)变为 ReLU。
    如果alpha_i>0，则PReLU(x)变为Leaky ReLU。
    如果alpha_i是可学习的参数，则PReLU(x)为PReLU函数。
特点：
    在负值域，PReLU的斜率比较小，避免Dead ReLU问题
    与ELU相比，PReLU 在负值域是线性运算。尽管斜率很小，但不会趋于0。
