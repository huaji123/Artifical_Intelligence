# 神经网络模型

## 1、神经元类别

### 1.1 神经元

对于不同类型的神经元标记不同的颜色，可以更好地灾各种网络加购之间进行区分。但是这些神经元的工作方式却大同小异。

基本的人工神经网络神经元相当简单，可以在前馈人工神经网络架构里面找到。它可以和前一层神经网络层中的所有神经元相连。

每一个连接都有各自的权重，通常情况下是一些随机值（影响训练过程以及模型的性能），求和，加上偏置项（避免输出为0），输入激活函数，激活函数的输出就是神经元的输出

### 1.2 卷积神经元（Convolutional cells）

只跟前一神经细胞层的部分神经元连接，与特定范围内的神经元相连接，通常用来保存空间信息。对于图像数据、语音数据非常实用

### 1.3 解卷积神经元

通过跟下一层神经细胞层的连接来解码空间信息。与卷积神经元都是独立训练，每个副本都有自己的权重，但连接方式却完全相同

### 1.4 池化神经元和插值神经元（Pooling and interpolating cells)

经常和卷积神经元结合实用，不是真正意义的神经元。

池化神经元接受输入，保留需要的信息（像素），舍弃不需要的信息（像素）

插值神经元接受输入，映射出更多的信息，额外的信息都是按照某种方式制造出来的

### 1.5 均值神经元（Mean cells）和标准方差神经元（Standard deviation cells）【成对出现】

是一类用来描述数据概率分布的神经元。均值就是所有值的平均值，而标准方差描述的是这些数据偏离（两个方向）均值有多远。比如：一个用于图像处理的概率神经元可以包含一些信息，比如：在某个特定的像素里面有多少红色。举个例来说，均值可能是0.5，同时标准方差是0.2。当要从这些概率神经元取样的时候，你可以把这些值输入到一个高斯随机数生成器，这样就会生成一些分布在0.4和0.6之间的值；值离0.5越远，对应生成的概率也就越小。它们一般和前一神经元层或者下一神经元层是全连接，而且，它们没有偏差（bias）。

### 1.6 循环神经元（Recurrent cells）

不仅仅在神经细胞层之间有连接，而且在时间轴上也有相应的连接。每一个神经元内部都会保存它先前的值。它们跟一般的神经元一样更新，但是，具有额外的权重：与当前神经元之前值之间的权重，还有大多数情况下，与同一神经细胞层各个神经元之间的权重。当前值和存储的先前值之间权重的工作机制，与非永久性存储器（比如RAM）的工作机制很相似，继承了两个性质：

第一，维持一个特定的状态；
第二：如果不对其持续进行更新（输入），这个状态就会消失。

由于先前的值是通过激活函数得到的，而在每一次的更新时，都会把这个值和其它权重一起输入到激活函数，因此，信息会不断地流失。实际上，信息的保存率非常的低，以至于仅仅四次或者五次迭代更新过后，几乎之前所有的信息都会流失掉。

### 1.7 长短期记忆神经元（long short term memory cells）

用于克服循环神经元中信息快速流失的问题。LSTM可以存储四个状态：输出的当前和先前值，记忆神经元状态的当前值和先前值。它们都有三个门：输入门，输出门，遗忘门，还有常规输入。与这种类型的神经元细胞相连需要设置4个权重

这种运行机制的实现是通过把输入信息和一个在0到1之间的系数相乘，这个系数存储在当前门中。这样，输入门决定输入的信息有多少可以被叠加到当前门值。输出门决定有多少输出信息是可以传递到后面的神经网络中。遗忘门并不是和输出神经元的先前值相连接，而是，和前一记忆神经元相连接。它决定了保留多少记忆神经元最新的状态信息。因为没有和输出相连接，以及没有激活函数在这个循环中，因此只会有更少的信息流失。

### 1.8 门控循环神经元（Gated recurrent units cells）

是LSTM的变体。使用两个门（更新门和重置门）来抑制信息的流失

与LSTM的区别：1）GRU神经元没有被输出们保护的隐神经元； 2）GRU把输出们和遗忘门整合在了一起，形成了更新门。核心的思想就是如果你想要一些新的信息，那么你就可以遗忘掉一些陈旧的信息（反过来也可以）。

### 1.9 全连接层

### 1.10 卷积神经层

### 1.11 时间滞后连接

是指相连的神经元（通常是在同一个神经元层，甚至于一个神经元自己跟自己连接），它们不从前面的神经元层获取信息，而是从神经元层先前的状态获取信息。这使得暂时（时间上或者序列上）联系在一起的信息可以被存储起来。这些形式的连接经常被手工重新进行设置，从而可以清除神经网络的状态。和常规连接的主要区别是，这种连接会持续不断地改变，即便这个神经网络当前没有处于训练状态。

## 2、神经网络

### 2.1 前馈神经网络（FFNN）

前馈神经网络和感知机(perceptrons)非常简单，信息从前往后流动（分别对应输入和输出）。

一般在描述神经网络的时候，都是从它的层说起，即相互平行的输入层、隐含层或者输出层神经结构。单独的神经细胞层内部，神经元之间互不相连；而一般相邻的两个神经细胞层则是全连接（一层的每个神经元和另一层的每一个神经元相连）。给神经网络一对数据集（分别是“输入数据集”和“我们期望的输出数据集”），一般通过反向传播算法来训练前馈神经网络（FFNNs）。

这就是所谓的监督式学习。与此相反的是无监督学习：我们只给输入，然后让神经网络去寻找数据当中的规律。反向传播的误差往往是神经网络当前输出和给定输出之间差值的某种变体（比如MSE或者仅仅是差值的线性变化）。如果神经网络具有足够的隐层神经元，那么理论上它总是能够建立输入数据和输出数据之间的关系。在实践中，FFNN的使用具有很大的局限性，但是，它们通常和其它神经网络一起组合成新的架构。

### 2.2 径向基神经网络（RBF）

径向基函数核（Radial Basis Function, RBF kernel），也被称为高斯核（Gaussian kernel）或平方指数核（Squared Exponential., SE kernel） [1]  ，是常见的核函数（kernel function）。RBF核被应用各类核学习（kernel learning）算法中，包括支持向量机（Support Vector Machine, SVM）、高斯过程回归（Gaussian Process Regression, GPR）等。

是一种单隐层前馈神经网络，使用径向基函数作为隐层神经元激活函数，而输出层是对隐层神经元输出的线性组合。

### 2.3 霍普菲尔网络（HN：Hopfield network）

是一种每一个神经元都跟其他神经元相互连接的网络。每个神经元都在充当所有角色：训练前的每一个节点都是输入神经元，训练阶段是隐层神经元，输出阶段则是输出神经元。

该神经网络的训练，是先把神经元的值设置到期望模式，然后计算相应的权重。在这以后，权重将不会再改变了。一旦网络被训练包含一种或者多种模式，这个神经网络总是会收敛于其中的某一种学习到的模式，因为它只会在某一个状态才会稳定。值得注意的是，它并不一定遵从那个期望的状态（很遗憾，它并不是那个具有魔法的黑盒子）。它之所以会稳定下来，部分要归功于在训练期间整个网络的“能量（Energy）”或“温度（Temperature）”会逐渐地减少。每一个神经元的激活函数阈值都会被设置成这个温度的值，一旦神经元输入的总和超过了这个阈值，那么就会让当前神经元选择状态（通常是-1或1，有时也是0或1）。

可以多个神经元同步，也可以一个神经元一个神经元地对网络进行更新。一旦所有的神经元都已经被更新，并且它们再也没有改变，整个网络就算稳定（退火）了，那你就可以说这个网络已经收敛了。这种类型的网络被称为“联想记忆（associative memory）”，因为它们会收敛到和输入最相似的状态；比如，人类看到桌子的一半就可以想象出另外一半；与之相似，如果输入一半噪音+一半桌子，这个网络就能收敛到整张桌子。

### 2.4 马尔可夫链（MC：Markov Chain)

马尔可夫链或离散时间马尔可夫链(discrete time Markov Chain)在某种意义上是BMs和HNs的前身。

从我当前所处的节点开始，走到任意相邻节点的概率是多少呢？它们没有记忆（所谓的马尔可夫特性）：你所得到的每一个状态都完全依赖于前一个状态。尽管算不上神经网络，但它却跟神经网络类似，并且奠定了BM和HN的理论基础。跟BM、RBM、HN一样，MC并不总被认为是神经网络。此外，它也并不总是全连接的。

### 2.5 玻尔兹曼机（BM：Boltzmann machines）

与霍普费尔网络接近，区别：一些神经元作为输入神经元，剩余的则是作为隐神经元。

在整个神经网络更新过后，输入神经元成为输出神经元。刚开始神经元的权重都是随机的，通过反向传播（back-propagation）算法进行学习，或是最近常用的对比散度（contrastive divergence）算法（马尔可夫链用于计算两个信息增益之间的梯度）。

相比HN，大多数BM的神经元激活模式都是二元的。BM由MC训练获得，因而是一个随机网络。BM的训练和运行过程，跟HN大同小异：为输入神经元设好钳位值，而后让神经网络自行学习。因为这些神经元可能会得到任意的值，我们反复地在输入和输出神经元之间来回地进行计算。激活函数的激活受全局温度的控制，如果全局温度降低了，那么神经元的能量也会相应地降低。这个能量上的降低导致了它们激活模式的稳定。在正确的温度下，这个网络会抵达一个平衡状态。

### 2.6 受限玻尔兹曼机（RBM：Restricted Boltzmann machines）

与BM相似，区别在于：RBM更具实用价值，因为它们受到了更多的限制。它们不会随意在所有神经元间建立连接，而只在不同神经元群之间建立连接，因此任何输入神经元都不会同其他输入神经元相连，任何隐神经元也不会同其他隐神经元相连。

训练方式：前向通过数据之后再将这些数据反向传回。之后再使用前向和反向传播进行训练。











