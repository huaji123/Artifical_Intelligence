1.K-邻近算法
1.1 K-邻近算法简介
      1）定义：就是通过你的“邻居”来判断你属于哪个类
      2）如何计算你到你的“邻居”的距离
            一般时候，都是使用欧式距离：勾股定理
1.2 k近邻算法api初步使用
      1）sklearn
            优势：
            1.文档多，且规范
            2.包含的算法多
            3.实现起来容易
      2）sklearn中包含内容
            分类、聚类、回归
            特征工程
            模型选择、调优
      3）knn中的api
            sklearn.neighbors_KNeighborsClassifier(n_neighbors=5)
            参数：
                  n_neighbors --选定参考几个邻居
      4）机器学习中实现的过程
            1.实例化一个估计器
            2.使用fit方法进行训练
1.3 距离度量
      1）欧式距离
            通过距离平方值进行计算
      2）曼哈顿距离（Manhanttan Distance）:d12 = |x1-x2|+|y1-y2|
            通过距离的绝对值进行计算
      3）切比雪夫距离：d12 = max(|x1-x2|,|y1-y2|)
            维度的最大值进行计算
      4）闵可夫斯基距离：
            p = 1时，就是曼哈顿距离
            p = 2时，就是欧式距离
            p = 3时，就是切比雪夫距离
      5）标准化欧式距离：d12 = 根号内∑((x1k-x2k)/Sk)²; Sk为标准差
            在计算过程中添加了标准差，对量刚数据进行处理
      6）余弦距离：cos = (a * b) / (|a| * |b|)
      7）汉明距离
      8）杰卡德距离
      9）马氏距离
1.4 k值选择
      k值过小：
            容易受到异常点的影响
            过拟合
      k值过大：
            受到样本均衡的问题
            欠拟合
      拓展
            近似误差--过拟合
            --在训练集上表现好，测试集表现不好--
            估计误差好才是真的好
1.5通用步骤
      1）计算距离
      2）升序排列
      3）取前k个
      4）加权平均

k近邻算法总结
      1）优点：
            简单有效
            重新训练代价低
            适合类域交叉样本
            适合大样本自动分类
      2）缺点：
            惰性学习
            类别评分不是规格化
            输出可解释性不强
            对不均衡的样本不擅长
            计算量较大

2. Kd树
2.1 什么是Kd树
      kd-tree（k-dimensional树的简称），是一种对k维空间中的实例点进行存储以便对其进行快速检索的树形数据结构。 [1]  主要应用于多维空间关键数据的搜索（如：范围搜索和最近邻搜索）。K-D树是二进制空间分割树的特殊的情况。
2.2 Kd树的原理
2.3 kd树的建立
      ①顺序选择，即按照数据的顺序一次在kd树中插入节点；
      ②选择待划分维数的中位数为划分的节点。在kd树的构建过程中，为了防止出现只有左子树或只有右子树的情况出现，通常对于每一个节点，选择样本中的中位数作为切分点。这样构建出来的kd树是平衡的
      注：平衡的kd树搜索时的效率未必是最优的
2.4 kd树的检索
      ①从根节点开始，将待检索的样本划分到对应区域中（在kd树形结构中，从根节点开始查找，知道叶子节点，将这样的查找序列存储到栈中
      ②以栈顶元素与待检索的样本之间的距离作为最短距离min_distance
      ③执行出栈操作：
            向上回溯，查找到父节点，若父节点与待检索样本之间的距离小于当前的距离，则替换
            以待检索的样本为圆心，以min_distance为半径画圆，若圆与父节点所在的平面相割，则需将父节点的另一棵子树进栈，重新执行以上的出栈操作，
      ④直到栈空
