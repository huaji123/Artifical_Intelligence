import paddle
from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier
import paddle.nn as N
import paddle.nn.functional as F
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
import seaborn as sns
import matplotlib.pyplot as plt
# 导入数据集
datasets = pd.read_excel('D:\Pycharm\workplace\AI\Dry_Bean_Dataset.xlsx')

# 将数据集分为训练集和测试集
data_train, data_test = train_test_split(datasets, test_size=0.2, train_size=0.8, random_state=None,
                                         shuffle=True)

# 将DataFrame格式转换为ndarray格式
datasets = datasets.values
data_train = data_train.values
data_test = data_test.values
datasets[:, :16].astype(np.float32)
data_train[:, :16].astype(np.float32)
data_test[:, :16].astype(np.float32)

le = preprocessing.LabelEncoder()

# 数据归一化处理
# 参数归一化函数
# 归一化时，需要把Dataframe格式转换成ndarray格式；
class StandardScaler:
    def __init__(self):
        self.mean_ = None
        self.scale_ = None

    def fit(self, X):
        # 根据训练数据集获得数据的均值和方差
        # print(X.shape[1])
        self.mean_ = np.array([np.mean(X[:, i]) for i in range(X.shape[1])])
        self.scale_ = np.array([np.std(X[:, i]) for i in range(X.shape[1])])
        return self

    def transform(self, X):
        # 将X根据Standardcaler进行均值方差归一化处理
        resX = np.empty(shape=X.shape, dtype=float)
        for col in range(X.shape[1]):
            resX[:, col] = (X[:, col] - self.mean_[col]) / (self.scale_[col])
        return resX


StandardScaler = StandardScaler()
StandardScaler.fit(datasets[:, :16])

x_train = StandardScaler.transform(data_train[:, :16])
x_test = StandardScaler.transform(data_test[:, :16])

data_train = np.c_[x_train, data_train[:, -1]]
data_test = np.c_[x_test, data_test[:, -1]]

# 存放精确度
accuracies = []
# 存放损失值
losses = []

batch_size = 20


def train(model):
    print('start training')
    model.train()
    EPOCH_NUM = 20
    optimizer = paddle.optimizer.Adam(learning_rate=0.01, parameters=model.parameters())
    for epoch in range(EPOCH_NUM):

        np.random.shuffle(data_train)

        mini_batches = [data_train[k: k + batch_size] for k in range(0, len(data_train), batch_size)]

        for batch_id, data in enumerate(mini_batches):
            features_np = np.array(data[:, :16], np.float32)
            labels_np = np.array(data[:, -1])
            print(labels_np)
            features = paddle.to_tensor(features_np)
            print(features)
            labels_np = le.fit_transform(labels_np)
            print(labels_np)
            labels = paddle.to_tensor(labels_np, np.float32)
            print(labels)
            y_pred = model(features)
            loss_fun = paddle.nn.CrossEntropyLoss(reduction=None)
            loss = loss_fun(y_pred, labels)
            avg_loss = paddle.mean(loss)
            if batch_id % 2000 == 0:
                print("epoch:{},batch_id:{},loss is:{:.4f}".format(epoch, batch_id, avg_loss))

            avg_loss.backward()
            optimizer.step()
            optimizer.clear_grad()
        model.eval()

        np.random.shuffle(data_test)

        mini_batches = [data_test[k: k + batch_size] for k in range(0, len(data_train), batch_size)]

        for batch_id, data in enumerate(mini_batches):
            features_np = np.array(data[:, :16], np.float32)
            labels_np = np.array(data[:, -1], np.array2string)
            features = paddle.to_tensor(features_np)
            # labels_np = le.fit_transform(labels_np)
            labels = paddle.to_tensor(labels_np)
            pred = F.softmax(features)

            loss_fun = paddle.nn.CrossEntropyLoss(reduction=None)
            loss = loss_fun(pred, labels)

            acc = paddle.metric.accuracy(pred, labels)

            accuracies.append(acc.numpy())

            losses.append(loss.numpy())

        print("[validation] accuracy/loss: {:.4f}/{:.4f}".format(np.mean(accuracies), np.mean(losses)))
        model.train

    paddle.save(model.state_dict(), '../DryBean.pdparams')


model = KNeighborsClassifier(n_neighbors=8, algorithm='kd_tree')

model.fit(data_train[:, :16], data_train[:, -1])

pre_test = model.predict(data_test[:, :16])

correct = np.count_nonzero((pre_test == data_test[:, -1]) == True)

print('准确率%.1f' % (100 * correct / (len(data_test[:, -1]))) + '%')
