# 降维

## 两种方式

### 1、特征选择

1、一般流程：

①生成子集：搜索特征子集，为评价函数提供特征子集

②评价函数：评价特征子集的好坏

③停止标准：与评价函数相关，一般是阈值，评价函数达到一定标准后就可停止搜索

④验证过程：在验证集上验证选出来的特征子集的有效性

2、三大类方法

①Filter（过滤法）：按照发散性或相关性对各个特征进行评分，设定阈值或者待选择特征的个数进行筛选

基本想法是：分别对每个特征 x_i ，计算 x_i 相对于类别标签 y 的信息量 S(i) ，得到 n 个结果。然后将 n 个 S(i) 按照从大到小排序，输出前 k 个特征。显然，这样复杂度大大降低。那么关键的问题就是使用什么样的方法来度量 S(i) ，我们的目标是选取与 y 关联最密切的一些 特征x_i 。

  1）Pearson相关系数：特征与特征之间的相关程度，缺点：只对线性关系敏感。
  
      from scipy.stats import pearsonr
      
      特征与特征相关性很高：
          
          （1）选取其中一个
          
          （2）加权求和
          
          （3）主成分分析：
                
                定义： 是一种统计方法。通过正交变换将一组可能存在相关性的变量转换为一组线性不相关的变量，转换后的这组变量叫主成分。
                
                原理：当两个变量之间有一定相关关系时，可以解释为这两个变量反映此课题的信息有一定的重叠。主成分分析是对于原先提出的所有变量，将重复的变量（关系紧密的变量）删去多余，建立尽可能少的新变量，使得这些新变量是两两不相关的，而且这些新变量在反映课题的信息方面尽可能保持原有的信息。
                
                基本思想：设法将原来众多具有一定相关性（比如P个指标），重新组合成一组新的互相无关的综合指标来代替原来的指标。
                
                    from sklearn.decomposition import PCA

  2）卡方验证：检验是检验类别型变量对类别型变量的相关性。假设自变量有N种取值，因变量有M种取值，考虑自变量等于i且因变量等于j的样本频数的观察值与期望的差距，构建统计量：x² = ∑[(A-E)²/E]
  
      from sklearn.feature_selection import SelectKBest

  3）互信息和最大信息系数：

  4）距离相关系数：

  5）方差选择法：低方差特征过滤
  
      from sklearn.feature_selection import VarianceThreshold

②Warpper（包装法）：根据目标函数（往往是预测效果评分），每次选择若干特征，或者排除若干特征

基本思想：基于hold-out方法，对于每一个待选的特征子集，都在训练集上训练一遍模型，然后在测试集上根据误差大小选择出特征子集。需要先选定特定算法，通常选用普遍效果较好的算法， 例如Random Forest， SVM， kNN等等。

启发式优化算法：GA, PSO, DE, ABC

主要方法：recursive feature elimination algorithm(递归特征消除算法)

③Embedded（嵌入法）：先使用某些机器学习的模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征（类似于Filter，只不过系数是通过训练得来的）

 主要方法：
        
        （1）决策树
        
        （2）正则化
        
        （3）深度学习

### 2、特征抽取



### 3、常用的特征提取方法有哪些？

常用的方法有主成分分析（PCA），独立成分分析（ICA），线性判别分析（LDA）一般数据是有类别的，最好先考虑用LDA降维。也可先用小幅度的PCA降维消除噪声再用LDA降维，若训练数据没有类别优先考虑PCA。

特征提取是由原始输入形成较少的新特征，它会破坏数据的分布，为了使得训练出的模型更加健壮，若不是数据量很大特征种类很多，一般不要用特征提取。

1.PCA

作为一个非监督学习的降维方法，它只需要特征值分解，就可以对数据进行压缩，去噪。因此在实际场景应用很广泛。为了克服PCA的一些缺点，出现了很多PCA的变种，比如为解决非线性降维的KPCA，还有解决内存限制的增量PCA方法Incremental PCA，以及解决稀疏数据降维的PCA方法Sparse PCA等。

PCA是最常用的线性降维方法，它的目标是通过某种线性投影，将高维的数据映射到低维的空间中表示，并期望在所投影的维度上数据的方差最大（样本的分布最散乱）以使用较少的数据维度同时保留住较多的原数据点的特征。

PCA的优缺点分析：

优点：

第一、仅仅需要以方差衡量信息量，不受数据集以外的因素影响。第二、各主成分之间正交，可消除原始数据成分间的相互影响的因素。第三、计算方法简单，主要运算是特征值分解，易于实现。

缺点：

第一、提取出的各个特征维度的含义具有一定的模糊性，不如原始样本特征的解释性强。第二、PCA会消除一些类信息，但是方差小的非主成分也可能含有对样本差异的重要信息，因降维丢弃可能对后续数据处理有影响。

 2.LDA

LDA是一种监督学习的降维技术，也就是说它的数据集的每个样本是有类别输出的。LDA的思想可以用一句话概括，就是“投影后类内方差最小，类间方差最大”。什么意思呢？ 我们要将数据在低维度上进行投影，投影后希望每一种类别数据的投影点尽可能的接近，而不同类别的数据的类别中心之间的距离尽可能的大。

LDA的优缺点分析：

LDA算法的主要优点有：

1）在降维过程中可以使用类别的先验知识经验，而像PCA这样的无监督学习则无法使用类别先验知识。

2）LDA在样本分类信息依赖均值而不是方差的时候，比PCA之类的算法较优。

LDA算法的主要缺点有：

1）LDA不适合对非高斯分布样本进行降维，PCA也有这个问题。

2）LDA降维最多降到类别数k-1的维数，如果我们降维的维度大于k-1，则不能使用LDA。当然目前有一些LDA的进化版算法可以绕过这个问题。

3）LDA在样本分类信息依赖方差而不是均值的时候，降维效果不好。

4）LDA可能过度拟合数据。
