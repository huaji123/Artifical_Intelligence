# 降维

## 两种方式

### 1、特征选择

1、一般流程：

①生成子集：搜索特征子集，为评价函数提供特征子集

②评价函数：评价特征子集的好坏

③停止标准：与评价函数相关，一般是阈值，评价函数达到一定标准后就可停止搜索

④验证过程：在验证集上验证选出来的特征子集的有效性

2、三大类方法

①Filter（过滤法）：按照发散性或相关性对各个特征进行评分，设定阈值或者待选择特征的个数进行筛选

基本想法是：分别对每个特征 x_i ，计算 x_i 相对于类别标签 y 的信息量 S(i) ，得到 n 个结果。然后将 n 个 S(i) 按照从大到小排序，输出前 k 个特征。显然，这样复杂度大大降低。那么关键的问题就是使用什么样的方法来度量 S(i) ，我们的目标是选取与 y 关联最密切的一些 特征x_i 。

  1）Pearson相关系数：特征与特征之间的相关程度，缺点：只对线性关系敏感。
  
      from scipy.stats import pearsonr
      
      特征与特征相关性很高：
          
          （1）选取其中一个
          
          （2）加权求和
          
          （3）主成分分析：
                
                定义： 是一种统计方法。通过正交变换将一组可能存在相关性的变量转换为一组线性不相关的变量，转换后的这组变量叫主成分。
                
                原理：当两个变量之间有一定相关关系时，可以解释为这两个变量反映此课题的信息有一定的重叠。主成分分析是对于原先提出的所有变量，将重复的变量（关系紧密的变量）删去多余，建立尽可能少的新变量，使得这些新变量是两两不相关的，而且这些新变量在反映课题的信息方面尽可能保持原有的信息。
                
                基本思想：设法将原来众多具有一定相关性（比如P个指标），重新组合成一组新的互相无关的综合指标来代替原来的指标。
                
                    from sklearn.decomposition import PCA

  2）卡方验证：检验是检验类别型变量对类别型变量的相关性。假设自变量有N种取值，因变量有M种取值，考虑自变量等于i且因变量等于j的样本频数的观察值与期望的差距，构建统计量：x² = ∑[(A-E)²/E]
  
      from sklearn.feature_selection import SelectKBest

  3）互信息和最大信息系数：

  4）距离相关系数：

  5）方差选择法：低方差特征过滤
  
      from sklearn.feature_selection import VarianceThreshold

②Warpper（包装法）：根据目标函数（往往是预测效果评分），每次选择若干特征，或者排除若干特征

基本思想：基于hold-out方法，对于每一个待选的特征子集，都在训练集上训练一遍模型，然后在测试集上根据误差大小选择出特征子集。需要先选定特定算法，通常选用普遍效果较好的算法， 例如Random Forest， SVM， kNN等等。

③Embedded（嵌入法）：先使用某些机器学习的模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征（类似于Filter，只不过系数是通过训练得来的）

### 2、特征抽取
